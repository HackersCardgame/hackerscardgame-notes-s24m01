OO @®«
Figure 5 | Go knowledge learned by AlphaGo Zero. a, Five human joseki
(common corner sequences) discovered during AlphaGo Zero training.
The associated timestamps indicate the first time each sequence occurred
(taking account of rotation and reflection) during self-play training.
Extended Data Figure 2 provides the frequency of occurence over training
for each sequence. b, Five joseki favoured at different stages of self-play
training, Each displayed corner sequence was played with the greatest
frequency, among all corner sequences, during an iteration of self-play
training. The timestamp of that iteration is indicated on the timeline. At
10 ha weak corner move was preferred. At 47 h the 3-3 invasion was most
frequently played. This joseki is also common in human professional play;

either separate policy and value networks, as were used in AlphaGo
Lee, or combined policy and value networks, as used in AlphaGo Zero;
and using either the convolutional network architecture from AlphaGo
Lee or the residual network architecture from AlphaGo Zero. Each
network was trained to minimize the same loss function (equation (1)),
using a fixed dataset of self-play games generated by AlphaGo Zero
after 72h of self-play training. Using a residual network was more
accurate, achieved lower error and improved performance in AlphaGo
by over 600 Elo. Combining policy and value together into a single
network slightly reduced the move prediction accuracy, but reduced the
value error and boosted playing performance in AlphaGo by around

however AlphaGo Zero later discovered and preferred a new variation.
Extended Data Figure 3 provides the frequency of occurence over time
for all five sequences and the new variation. c, The first 80 moves of three
self-play games that were played at different stages of training, using 1,600
simulations (around 0.45) per search. At 3h, the game focuses greedily
on capturing stones, much like a human beginner. At 19h, the game
exhibits the fundamentals of life-and-death, influence and territory. At
70h, the game is remarkably balanced, involving multiple battles and a
complicated ko fight, eventually resolving into a half-point win for white.
See Supplementary Information for the full games.

another 600 Elo. This is partly due to improved computational effi-
ciency, but more importantly the dual objective regularizes the network
toa common representation that supports multiple use cases,

Knowledge learned by AlphaGo Zero
AlphaGo Zero discovered a remarkable level of Go knowledge dur-
ing its self-play training process. This included not only fundamental
elements of human Go knowledge, but also non-standard strategies
beyond the scope of traditional Go knowledge.

Figure 5 shows a timeline indicating when professional joseki
(corner sequences) were discovered (Fig. 5a and Extended Data
