(4) A fully connected linear layer to a hidden layer of size 256

(5) A rectifier nonlinearity

(6) A fully connected linear layer to a scalar

(7) A tanh nonlinearity outputting a scalar in the range [—1, 1]

The overall network depth, in the 20- or 40-block network, is 39 or 79 param-

eterized layers, respectively, for the residual tower, plus an additional 2 layers for
the policy head and 3 layers for the value head.

We note that a different variant of residual networks was simultaneously applied
to computer Go* and achieved an amateur dan-level performance; however, this

was restricted to a single-headed policy network trained solely by supervised
learning.

Neural network architecture comparison. Figure 4 shows the results of a com-
parison between network architectures. Specifically, we compared four different
neural networks:

(1) dual-res: the network contains a 20-block residual tower, as described above,

followed by both a policy head and a value head. This is the architecture used in
AlphaGo Zero.

(2) sep-res: the network contains two 20-block residual towers. The first tower

is followed by a policy head and the second tower is followed by a value head.

(3) dual-conv: the network contains a non-residual tower of 12 convolutional

blocks, followed by both a policy head and a value head.

(4) sep-conv: the network contains two non-residual towers of 12 convolutional

blocks. The first tower is followed by a policy head and the second tower is followed
bya value head. This is the architecture used in AlphaGo Lee.

Each network was trained on a fixed dataset containing the final 2 million

games of self-play data generated by a previous run of AlphaGo Zero, using
stochastic gradient descent with the annealing rate, momentum and regulariza-
tion hyperparameters described for the supervised learning experiment; however,
cross-entropy and MSE components were weighted equally, since more data was
available.

Evaluation. We evaluated the relative strength of AlphaGo Zero (Figs 3a, 6) by
measuring the Elo rating of each player. We estimate the probability that player a
will defeat player b by a logistic function P(a defeats b) =

1 sand
1+ exp(celoCe(b) — e(a))

estimate the ratings e(-) by Bayesian logistic regression, computed by the BayesElo

program” using the standard constant Cej)= 1/400.
Elo ratings were computed from the results of a5 s per move tournament

between AlphaGo Zero, AlphaGo Master, AlphaGo Lee and AlphaGo Fan. The
raw neural network from AlphaGo Zero was also included in the tournament.
The Elo ratings of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored

to the tournament values from previous work!”, and correspond to the players
reported in that work. The results of the matches of AlphaGo Fan against Fan
Hui and AlphaGo Lee against Lee Sedol were also included to ground the scale

to human references, as otherwise the Elo ratings of AlphaGo are unrealistically
high due to self-play bias.

The Elo ratings in Figs 3a, 4a, 6a were computed from the results of evaluation

games between each iteration of player ag, during self-play training, Further eval-
uations were also performed against baseline players with Elo ratings anchored to

the previously published values’,
We measured the head-to-head performance of AlphaGo Zero against AlphaGo

Lee, and the 40-block instance of AlphaGo Zero against AlphaGo Master, using the
same player and match conditions that were used against Lee Sedol in Seoul, 2016.
Each player received 2h of thinking time plus 3 byoyomi periods of 60s per move.
All games were scored using Chinese rules with a komi of 7.5 points.

Data availability. The datasets used for validation and testing are the GoKifu
dataset (available from http://gokifu.com/) and the KGS dataset (available from
https://u-go.net/gamerecords/).

Barto, A. G. & Duff, M. Monte Carlo matrix inversion and reinforcement
learning. Adv. Neural Inf. Process. Syst. 6, 687-694 (1994).

Singh, S. P. & Sutton, R. S. Reinforcement learning with replacing eligibility
traces. Mach. Learn. 22, 123-158 (1996).

37.

Lagoudakis, M. G. & Parr, R. Reinforcement leaming as classification
leveraging modem classifiers. In Proc. 20th Int. Conf. Mach. Learn. 424-43 1
(2003).

Scherrer, B, Ghavamzadeh, M., Gabillon, V. Lesner, B. & Geist M. Approximate
modified policy iteration and its application to the game of Tetris. J. Mach.
Learn. Res. 16, 1629-1676 (2015).

Littman, M. L Markov games as a framework for multi-agent reinforcement
learning. In Proc. 11th Int. Conf. Mach. Learn. 157-163 (1994).

Enzenberger, M. The integration of a priori knowledge into a Go playing neural
network. http://www.cgl.ucsf.edu/go/Programs/neurogo-html/neurogo.htm|
(1996).

Enzenberger, M. in Advances in Computer Games (eds Van Den Herik, H. J., lida,
H. & Heinz, E. A) 97-108 (2003).

Sutton, R. Learning to predict by the method of temporal differences. Mach.
Learn. 3, 9-44 (1988).

Schraudolph, N. N., Dayan, P. & Sejnowski, T J. Temporal difference learning of
position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817-824
(1994).

Silver, D,, Sutton, R. & Miller, M. Temporal-difference search in computer Go.
Mach. Learn. 87, 183-219 (2012).

Silver, D. Reinforcement Learning and Simulation-Based Search in Computer Go.
PhD thesis, Univ. Alberta, Edmonton, Canada (2009).

Gelly, S & Silver, D. Monte-Carlo tree search and rapid action value estimation
in computer Go. Artif. Intell. 175, 1856-1875 (2011).

Coulom, R. Computing Elo ratings of move patterns in the game of Go. Int
Comput. Games Assoc. J. 30, 198-208 (2007).

Gelly, S, Wang, Y., Munos, R. & Teytaud, O. Modification of UCT with patterns in
Monte-Carlo Go. Report No. 6062 (INRIA, 2006).

Baxter, J., Tridgell, A & Weaver, L. Learning to play chess using temporal
differences. Mach. Learn. 40, 243-263 (2000).

Veness, J., Silver, D., Blair, A & Uther, W. Bootstrapping from game tree search.
In Adv. Neural Inf. Process. Syst. 1937-1945 (2009).

Lai, M. Giraffe: Using Deep Reinforcement Learning to Play Chess. MSc thesis,
Imperial College London (2015).

Schaeffer, J., Hlynka, M. & Jussila, V. Temporal difference learning applied to a
high-performance game-playing program. In Proc. 17th Int. Jt Conf. Artif. Intell.
Vol. 1529-534 (2001).

Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves
master-level play. Neural Comput. 6, 215-219 (1994).

Buro, M. From simple features to sophisticated evaluation functions. In Proc.
Ast Int Conf. Comput. Games 126-145 (1999).

Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241-275
(2002).

Moravéik, M. et al. DeepStack: expertlevel artificial intelligence in heads-up
no-limit poker. Science 356, 508-513 (2017).

Tesauro, G & Galperin, G. On-line policy improvement using Monte-Carlo
search. In Adv. Neural Inf. Process. Syst 1068-1074 (1996).

Tesauro, G. Neurogammon: a neural-network backgammon program. In Proc.
Int. Jt Conf. Neural Netw. Vol. 3, 33-39 (1990).

Samuel, A. L. Some studies in machine learning using the game of checkers II -
recent progress. IBM J. Res. Develop. 11, 601-617 (1967).

Kober, J., Bagnell, J. A. & Peters, J. Reinforcement learning in robotics: a survey.
Int. J. Robot. Res. 32, 1238-1274 (2013).

Zhang, W. & Dietterich, T G. A reinforcement learning approach to job-shop
scheduling. In Proc. 14th Int. Jt Conf. Artif. Intell. 1114-1120 (1995).

Cazenave, T, Balbo, F. & Pinson, S. Using a Monte-Carlo approach for bus
regulation. In Int IEEE Conf. Intell. Transport. Syst. 1-6 (2009).

Evans, R. & Gao, J. Deepmind Al reduces Google data centre cooling bill by
40%. https: //deepmind.com/blog/deepmind-ai-reduces-google-data-centre-
cooling-bill-40/ (2016).

Abe, N. et al. Empirical comparison of various reinforcement learning strategies
for sequential targeted marketing. In IEEE Int Conf. Data Mining 3-10 (2002).
Silver, D., Newnham, L, Barker, D., Weller, S. & McFall, J. Concurrent
reinforcement learning from customer interactions. In Proc. 30th Int. Cont.
Mach, Learn. Vol. 28 924-932 (2013).

Tromp, J. Tomp-Taylor rules. http://tromp.github.io/go.html (1995).

Miller, M. Computer Go. Artif. Intell. 134, 145-179 (2002).

Shahriari, B., Swersky, K, Wang, Z., Adams, R. R & de Freitas, N. Taking the
human out of the loop: a review of Bayesian optimization. Proc. IEEE 104,
148-175 (2016)

Segal, R. B. On the scalability of parallel UCT. Comput. Games 6515, 36-47
(2011).
