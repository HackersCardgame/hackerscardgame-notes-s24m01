Elo rating

== Reinforcement learning
= Supervised learning
eee AlphaGo Lee

0 10 20 30 40 50 60 70

Training time (h)

Prediction accuracy
on professional moves (%)

= Reinforcement learning
= Supervised learning

0.35

0.30

0.25

MSE of professional
game outcomes

0.20

0.15

0 10 20 30 40 50 60 70

Training time (h)

= Reinforcement learning
— Supervised learning

0 10 20 30 40 50 6d 70
Training time (h)

Figure 3 | Empirical evaluation of AlphaGo Zero. a, Performance of self-
play reinforcement learning. The plot shows the performance of each
MCTS player ag; from each iteration i of reinforcement learning in
AlphaGo Zero. Elo ratings were computed from evaluation games between
different players, using 0.4s of thinking time per move (see Methods). For
comparison, a similar player trained by supervised learning from human
data, using the KGS dataset, is also shown. b, Prediction accuracy on
human professional moves. The plot shows the accuracy of the neural
network f,,, at each iteration of self-play i, in predicting human
professional moves from the GoKifu dataset. The accuracy measures the

from 700,000 mini-batches of 2,048 positions. The neural network
contained 20 residual blocks (see Methods for further details).

Figure 3a shows the performance of AlphaGo Zero during self-play
reinforcement learning, as a function of training time, on an Elo scale’.
Learning progressed smoothly throughout training, and did not suffer
from the oscillations or catastrophic forgetting that have been suggested
in previous literature”***. Surprisingly, AlphaGo Zero outperformed
AlphaGo Lee after just 36 h. In comparison, AlphaGo Lee was trained
over several months. After 72h, we evaluated AlphaGo Zero against the
exact version of AlphaGo Lee that defeated Lee Sedol, under the same
2h time controls and match conditions that were used in the man-
machine match in Seoul (see Methods). AlphaGo Zero used a single
machine with 4 tensor processing units (TPUs)”’, whereas AlphaGo
Lee was distributed over many machines and used 48 TPUs. AlphaGo
Zero defeated AlphaGo Lee by 100 games to 0 (see Extended Data Fig. 1
and Supplementary Information).

a b
4,5005 0.53 5
0.52 4
4,000 =
>
gg
530
2 Be
® 35004 5 g
g gs
oo
2%
ao
)

3,000 +

2,500 J

Oo
S
“a

> g

5 ra Ss

$
ca

Ne
S
> g

Figure 4| Comparison of neural network architectures in AlphaGo
Zero and AlphaGo Lee. Comparison of neural network architectures
using either separate (sep) or combined policy and value (dual) networks,
and using either convolutional (conv) or residual (res) networks. The
combinations ‘dual-res’ and ‘sep-conv’ correspond to the neural network
architectures used in AlphaGo Zero and AlphaGo Lee, respectively. Each
network was trained on a fixed dataset generated by a previous run of

percentage of positions in which the neural network assigns the highest
probability to the human move. The accuracy ofa neural network trained
by supervised learning is also shown. c, Mean-squared error (MSE) of
human professional game outcomes. The plot shows the MSE of the neural
network f,,, at each iteration of self-play i, in predicting the outcome of
human professional games from the GoKifu dataset. The MSE is between
the actual outcome z € {—1, +1} and the neural network value v, scaled by
a factor of to the range of 0-1. The MSE ofa neural network trained by

supervised learning is also shown.

To assess the merits of self-play reinforcement learning, compared to
learning from human data, we trained a second neural network (using
the same architecture) to predict expert moves in the KGS Server data-
set; this achieved state-of-the-art prediction accuracy compared to pre-
vious work!?*°-33 (see Extended Data Tables 1 and 2 for current and
previous results, respectively). Supervised learning achieved a better
initial performance, and was better at predicting human professional
moves (Fig. 3). Notably, although supervised learning achieved higher
move prediction accuracy, the self-learned player performed much
better overall, defeating the human-trained player within the first 24h
of training. This suggests that AlphaGo Zero may be learning a strategy
that is qualitatively different to human play.

To separate the contributions of architecture and algorithm, we
compared the performance of the neural network architecture in
AlphaGo Zero with the previous neural network architecture used in
AlphaGo Lee (see Fig. 4). Four neural networks were created, using

c
0.205

0.195
0.18-

0.174

MSE of professional
game outcomes

0.16-

0.15)

AlphaGo Zero. a, Each trained network was combined with AlphaGo
Zeros search to obtain a different player. Elo ratings were computed from
evaluation games between these different players, using 5s of thinking
time per move. b, Prediction accuracy on human professional moves
(from the GoKifu dataset) for each network architecture. c MSE of human
professional game outcomes (from the GoKifu dataset) for each network
architecture.
