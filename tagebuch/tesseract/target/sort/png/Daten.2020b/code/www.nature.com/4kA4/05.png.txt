Elo rating

«++ AlphaGo Lee

— AlphaGo Zero 40 blocks
+++ AlphaGo Master

Elo rating

0. Me

25 30

Figure 6 | Performance of AlphaGo Zero. a, Learning curve for AlphaGo
Zero using a larger 40-block residual network over 40 days. The plot shows
the performance of each player ag, from each iteration i of our
reinforcement learning algorithm. Elo ratings were computed from
evaluation games between different players, using 0.4s per search (see
Methods). b, Final performance of AlphaGo Zero. AlphaGo Zero was
trained for 40 days using a 40-block residual neural network. The plot
shows the results of a tournament between: AlphaGo Zero, AlphaGo
Master (defeated top human professionals 60-0 in online games), AlphaGo

Fig, 2); ultimately AlphaGo Zero preferred new joseki variants that
were previously unknown (Fig. 5b and Extended Data Fig. 3). Figure 5c
shows several fast self-play games played at different stages of train-
ing (see Supplementary Information). Tournament length games
played at regular intervals throughout training are shown in Extended
Data Fig. 4 and in the Supplementary Information. AlphaGo Zero
rapidly progressed from entirely random moves towards a sophisti-
cated understanding of Go concepts, including fuseki (opening), tesuji
(tactics), life-and-death, ko (repeated board situations), yose
(endgame), capturing races, sente (initiative), shape, influence and
territory, all discovered from first principles. Surprisingly, shicho
(ladder’ capture sequences that may span the whole board)—one of
the first elements of Go knowledge learned by humans—were only
understood by AlphaGo Zero much later in training,

Final performance of AlphaGo Zero

We subsequently applied our reinforcement learning pipeline to a
second instance of AlphaGo Zero using a larger neural network and
over a longer duration. Training again started from completely random
behaviour and continued for approximately 40 days.

Over the course of training, 29 million games of self-play were gener-
ated. Parameters were updated from 3.1 million mini-batches of 2,048
positions each. The neural network contained 40 residual blocks The
learning curve is shown in Fig. 6a. Games played at regular intervals
throughout training are shown in Extended Data Fig. 5 and in the
Supplementary Information.

We evaluated the fully trained AlphaGo Zero using an internal
tournament against AlphaGo Fan, AlphaGo Lee and several previous
Go programs. We also played games against the strongest existing
program, AlphaGo Master—a program based on the algorithm and
architecture presented in this paper but using human data and fea-
tures (see Methods)—which defeated the strongest human professional
players 60-0 in online games in January 2017°", In our evaluation, all
programs were allowed 5s of thinking time per move; AlphaGo Zero
and AlphaGo Master each played ona single machine with 4 TPUs;
AlphaGo Fan and AlphaGo Lee were distributed over 176 GPUs and
48 TPUs, respectively. We also included a player based solely on the raw
neural network of AlphaGo Zero; this player simply selected the move
with maximum probability.

Figure 6b shows the performance of each program on an Elo scale.
The raw neural network, without using any lookahead, achieved an Elo
rating of 3,055. AlphaGo Zero achieved a rating of 5,185, compared

40 oe FBX con &

x,
Cyt.
Sg LESS

&

Lee (defeated Lee Sedol), AlphaGo Fan (defeated Fan Hui), as well as
previous Go programs Crazy Stone, Pachi and GnuGo. Each program was
given 5s of thinking time per move. AlphaGo Zero and AlphaGo Master
played on a single machine on the Google Cloud; AlphaGo Fan and
AlphaGo Lee were distributed over many machines. The raw neural
network from AlphaGo Zero is also included, which directly selects the
move a with maximum probability p,, without using MCTS. Programs
were evaluated on an Elo scale”: a 200-point gap corresponds to a 75%
probability of winning,

to 4,858 for AlphaGo Master, 3,739 for AlphaGo Lee and 3,144 for
AlphaGo Fan.

Finally, we evaluated AlphaGo Zero head to head against AlphaGo
Master in a 100-game match with 2-h time controls. AlphaGo Zero
won by 89 games to 11 (see Extended Data Fig. 6 and Supplementary
Information).

Conclusion
Our results comprehensively demonstrate that a pure reinforcement
learning approach is fully feasible, even in the most challenging of
domains: it is possible to train to superhuman level, without human
examples or guidance, given no knowledge of the domain beyond basic
rules. Furthermore, a pure reinforcement learning approach requires
just a few more hours to train, and achieves much better asymptotic
erformance, compared to training on human expert data. Using this
approach, AlphaGo Zero defeated the strongest previous versions of
AlphaGo, which were trained from human data using handcrafted fea-
tures, by a large margin.
Humankind has accumulated Go knowledge from millions of games
played over thousands of years, collectively distilled into patterns, prov-
erbs and books. In the space of a few days, starting tabula rasa, AlphaGo
Zero was able to rediscover much of this Go knowledge, as well as novel
strategies that provide new insights into the oldest of games.

Online Content Methods, along with any additional Extended Data display items and
Source Data, are available in the online version of the paper; references unique to
these sections appear only in the online paper.

Received 7 April; accepted 13 September 2017.

1. Friedman, J., Hastie, T & Tibshirani, R. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction (Springer, 2009).

2. LeCun, Y., Bengio, ¥. & Hinton, G. Deep learning, Nature 521, 436-444 (2015).

3. Krizhevsky, A, Sutskever, |. & Hinton, G. ImageNet classification with deep

convolutional neural networks. In Adv. Neural Inf. Process. Syst. Vol. 25

(eds Pereira, F, Burges, C. J. C., Bottou, L & Weinberger, K. Q.) 1097-1105

(2012).

He, K, Zhang, X., Ren, S & Sun, J. Deep residual learning for image recognition

In Proc. 29th IEEE Conf. Comput. Vis. Pattern Recognit. 770-778 (2016).

5. Hayes-Roth, F, Waterman, D. & Lenat, D. Building Expert Systems (Addison-
Wesley, 1984).

6. Mnih, V. et al. Human-level control through deep reinforcement learning.
Nature 518, 529-533 (2015).

7. Guo, X, Singh, S. P, Lee, H., Lewis, R. L & Wang, X. Deep learning for real-time
Atari game play using offline Monte-Carlo tree search planning. In Adv. Neural
Inf. Process. Syst. Vol. 27 (eds Ghahramani, Z, Welling, M., Cortes, C., Lawrence,
N. D. & Weinberger, K. Q) 3338-3346 (2014).
