Mnih, V. et al. Asynchronous methods for deep reinforcement learning. In

Proc. 33rd Int. Conf. Mach. Learn. Vol. 48 (eds Balcan, M. F. & Weinberger, K. Q.)
1928-1937 (2016).

Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks.
In 5th Int. Conf. Learn. Representations (2017).

Dosovitskiy, A. & Koltun, V. Learning to act by predicting the future. In 5th Int
Conf. Learn. Representations (2017).

Mandziuk, J. in Challenges for Computational Intelligence (Duch, W. &
Maridziuk, J.) 407-442 (Springer, 2007).

Silver, D. et al. Mastering the game of Go with deep neural networks and tree
search. Nature 529, 484-489 (2016).

Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree
search. In 5th Int. Conf. Computers and Games (eds Ciancarini, P. & van den
Herik, H. J.) 72-83 (2006).

Kocsis, L. & Szepesvari, C. Bandit based Monte-Carlo planning. In 15th Eu.
Conf. Mach. Learn. 282-293 (2006).

Browne, C. et al. A survey of Monte Carlo tree search methods. IEEE Trans.
Comput Intell. Al Games 4, 1-49 (2012).

Fukushima, K. Neocognitron: a self organizing neural network model for a
mechanism of pattern recognition unaffected by shift in position. Biol. Cybern.
36, 193-202 (1980).

LeCun, Y. & Bengio, Y. in The Handbook of Brain Theory and Neural Networks

Ch. 3 (ed. Arbib, M.) 276-278 (MIT Press, 1995).

loffe, S. & Szegedy, C. Batch normalization: accelerating deep network training
by reducing internal covariate shift. In Proc. 32nd Int. Conf. Mach. Learn. Vol. 37
448-456 (2015).

Hahnloser, R. H. R, Sarpeshkar, R., Mahowald, M. A, Douglas, R. J. &

Seung, H. S Digital selection and analogue amplification coexist in a
cortex-inspired silicon circuit Nature 405, 947-951 (2000).

Howard, R. Dynamic Programming and Markov Processes (MIT Press, 1960).
Sutton, R & Barto, A. Reinforcement Learning: an Introduction (MIT Press,
1998).

Bertsekas, D. P. Approximate policy iteration: a survey and some new methods.
J. Control Theory Appl. 9, 310-335 (2011).

Scherrer, B. Approximate policy iteration schemes: a comparison. In Proc. 31st
Int. Conf. Mach. Learn. Vol. 32 1314-1322 (2014).

Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell.
61, 203-230 (2011).

Coulom, R Whole-history rating: a Bayesian rating system for players of
time-varying strength. In Int. Conf. Comput. Games (eds van den Herik, H. J., Xu,
X. Ma, Z. & Winands, M. H. M.) Vol. 5131 113-124 (Springer, 2008).

Laurent, G. J., Matignon, L. & Le Fort-Piat, N. The world of independent learners
is not Markovian. Int. J. Knowledge-Based Intelligent Engineering Systems 15,
55-64 (2011),

27. Foerster, J. N. etal. Stabilising experience replay for deep multi-agent reinforcement
learning In Proc. 34th Int. Conf. Mach. Lear. Vol. 70 1146-1155 (2017).

28. Heinrich, J. & Silver, D. Deep reinforcement learning from self-play in
imperfect-information games. In NIPS Deep Reinforcement Learning Workshop
(2016),

29. Jouppi, N. P etal. In-datacenter performance analysis of a Tensor
Processing Unit. Proc. 44th Annu. Int. Symp. Comp. Architecture Vol. 17 1-12
(2017)

30. Maddison, C. J., Huang A., Sutskever, |. & Silver, D. Move evaluation in Go
using deep convolutional neural networks. In 3rd Int. Conf. Learn.
Representations. (2015).

31. Clark, C. & Storkey, A J. Training deep convolutional neural networks
to play Go. In Proc. 32nd Int. Conf. Mach. Learn. Vol. 37 1766-1774
(2015),

32. Tian, Y. & Zhu, Y. Better computer Go player with neural network and long-term
prediction. In 4th Int. Conf. Learn. Representations (2016).

33. Cazenave, T Residual networks for computer Go. /EEE Trans. Comput. Intell. Al

Games https://doi.org/10.1109/TCIAIG.2017.2681042 (2017).

Huang, A. AlphaGo master online series of games. https://deepmind.com/

research/AlphaGo/match-archive/master (2017).

34,

Supplementary Information is available in the online version of the paper.

Acknowledgements We thank A. Cain for work on the visuals; A. Barreto,
G. Ostrovski, T. Ewalds, T. Schaul, J. Oh and N. Heess for reviewing the paper;
and the rest of the DeepMind team for their support.

Author Contributions D.S., J.S., K.S., LA. A.G., L.S. and TH. designed and
implemented the reinforcement learning algorithm in AlphaGo Zero. AH., J.S.,
MLL. and D.S. designed and implemented the search in AlphaGo Zero. L.B.,

JIS., AH., F.H., TH., Y.C. and D.S. designed and implemented the evaluation
framework for AlphaGo Zero. D.S., AB., F.H.,AG., TL. TG., L.S., G.v.d.D. and D.H.
managed and advised on the project. D.S., TG. and A.G. wrote the paper.

Author Information Reprints and permissions information is available at
www.nature.com/reprints. The authors declare no competing financial
interests. Readers are welcome to comment on the online version of the paper.
Publisher's note: Springer Nature remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations. Correspondence and
requests for materials should be addressed to D.S. (davidsilver@google.com).

Reviewer Information Nature thanks S. Singh and the other anonymous
reviewer(s) for their contribution to the peer review of this work.
